{"code":"/*\r\n * Copyright (c) Meta Platforms, Inc. and affiliates.\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *     http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n\r\n#include <folly/fibers/Baton.h>\r\n\r\n#include <chrono>\r\n\r\n#include <folly/detail/MemoryIdler.h>\r\n#include <folly/fibers/FiberManagerInternal.h>\r\n#include <folly/portability/Asm.h>\r\n\r\nnamespace folly {\r\nnamespace fibers {\r\n\r\nusing folly::detail::futexWake;\r\n\r\nvoid Baton::setWaiter(Waiter& waiter) {\r\n  auto curr_waiter = waiter_.load();\r\n  do {\r\n    if (LIKELY(curr_waiter == NO_WAITER)) {\r\n      continue;\r\n    } else if (curr_waiter == POSTED || curr_waiter == TIMEOUT) {\r\n      waiter.post();\r\n      break;\r\n    } else {\r\n      throw std::logic_error(\"Some waiter is already waiting on this Baton.\");\r\n    }\r\n  } while (!waiter_.compare_exchange_weak(\r\n      curr_waiter, reinterpret_cast<intptr_t>(&waiter)));\r\n}\r\n\r\nvoid Baton::wait() {\r\n  wait([]() {});\r\n}\r\n\r\nvoid Baton::wait(TimeoutHandler& timeoutHandler) {\r\n  auto timeoutFunc = [this] {\r\n    if (!try_wait()) {\r\n      postHelper(TIMEOUT);\r\n    }\r\n  };\r\n  timeoutHandler.timeoutFunc_ = std::ref(timeoutFunc);\r\n  timeoutHandler.fiberManager_ = FiberManager::getFiberManagerUnsafe();\r\n  wait();\r\n  timeoutHandler.cancelTimeout();\r\n}\r\n\r\nvoid Baton::waitThread() {\r\n  auto waiter = waiter_.load();\r\n\r\n  auto waitStart = std::chrono::steady_clock::now();\r\n\r\n  if (LIKELY(\r\n          waiter == NO_WAITER &&\r\n          waiter_.compare_exchange_strong(waiter, THREAD_WAITING))) {\r\n    do {\r\n      folly::detail::MemoryIdler::futexWait(\r\n          futex_.futex, uint32_t(THREAD_WAITING));\r\n      waiter = waiter_.load(std::memory_order_acquire);\r\n    } while (waiter == THREAD_WAITING);\r\n  }\r\n\r\n  folly::async_tracing::logBlockingOperation(\r\n      std::chrono::duration_cast<std::chrono::milliseconds>(\r\n          std::chrono::steady_clock::now() - waitStart));\r\n\r\n  if (LIKELY(waiter == POSTED)) {\r\n    return;\r\n  }\r\n\r\n  // Handle errors\r\n  if (waiter == TIMEOUT) {\r\n    throw std::logic_error(\"Thread baton can't have timeout status\");\r\n  }\r\n  if (waiter == THREAD_WAITING) {\r\n    throw std::logic_error(\"Other thread is already waiting on this baton\");\r\n  }\r\n  throw std::logic_error(\"Other waiter is already waiting on this baton\");\r\n}\r\n\r\nvoid Baton::post() {\r\n  postHelper(POSTED);\r\n}\r\n\r\nvoid Baton::postHelper(intptr_t new_value) {\r\n  auto waiter = waiter_.load();\r\n\r\n  do {\r\n    if (waiter == THREAD_WAITING) {\r\n      assert(new_value == POSTED);\r\n\r\n      return postThread();\r\n    }\r\n\r\n    if (waiter == POSTED) {\r\n      return;\r\n    }\r\n  } while (!waiter_.compare_exchange_weak(waiter, new_value));\r\n\r\n  if (waiter != NO_WAITER && waiter != TIMEOUT) {\r\n    reinterpret_cast<Waiter*>(waiter)->post();\r\n  }\r\n}\r\n\r\nbool Baton::try_wait() {\r\n  return ready();\r\n}\r\n\r\nvoid Baton::postThread() {\r\n  auto expected = THREAD_WAITING;\r\n\r\n  auto* futex = &futex_.futex;\r\n  if (!waiter_.compare_exchange_strong(expected, POSTED)) {\r\n    return;\r\n  }\r\n  futexWake(futex, 1);\r\n}\r\n\r\nvoid Baton::reset() {\r\n  waiter_.store(NO_WAITER, std::memory_order_relaxed);\r\n}\r\n\r\nvoid Baton::TimeoutHandler::scheduleTimeout(std::chrono::milliseconds timeout) {\r\n  assert(fiberManager_ != nullptr);\r\n  assert(timeoutFunc_ != nullptr);\r\n\r\n  if (timeout.count() > 0) {\r\n    fiberManager_->loopController_->timer()->scheduleTimeout(this, timeout);\r\n  }\r\n}\r\n\r\n} // namespace fibers\r\n} // namespace folly\r\n","text":"a.cpp"}
{"code":"/*\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/fibers/BatchSemaphore.h>\n\nnamespace folly {\nnamespace fibers {\n\nvoid BatchSemaphore::signal(int64_t tokens) {\n  signalSlow(tokens);\n}\n\nvoid BatchSemaphore::wait(int64_t tokens) {\n  wait_common(tokens);\n}\n\nbool BatchSemaphore::try_wait(Waiter& waiter, int64_t tokens) {\n  return try_wait_common(waiter, tokens);\n}\n\nbool BatchSemaphore::try_wait(int64_t tokens) {\n  return try_wait_common(tokens);\n}\n\n#if FOLLY_HAS_COROUTINES\n\ncoro::Task<void> BatchSemaphore::co_wait(int64_t tokens) {\n  co_await co_wait_common(tokens);\n}\n\n#endif\n\nSemiFuture<Unit> BatchSemaphore::future_wait(int64_t tokens) {\n  return future_wait_common(tokens);\n}\n\n} // namespace fibers\n} // namespace folly\n","text":"BatchSemaphore.cpp"}
{"code":"/*\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/fibers/Baton.h>\n\n#include <chrono>\n\n#include <folly/detail/MemoryIdler.h>\n#include <folly/fibers/FiberManagerInternal.h>\n#include <folly/portability/Asm.h>\n\nnamespace folly {\nnamespace fibers {\n\nusing folly::detail::futexWake;\n\nvoid Baton::setWaiter(Waiter& waiter) {\n  auto curr_waiter = waiter_.load();\n  do {\n    if (LIKELY(curr_waiter == NO_WAITER)) {\n      continue;\n    } else if (curr_waiter == POSTED || curr_waiter == TIMEOUT) {\n      waiter.post();\n      break;\n    } else {\n      throw std::logic_error(\"Some waiter is already waiting on this Baton.\");\n    }\n  } while (!waiter_.compare_exchange_weak(\n      curr_waiter, reinterpret_cast<intptr_t>(&waiter)));\n}\n\nvoid Baton::wait() {\n  wait([]() {});\n}\n\nvoid Baton::wait(TimeoutHandler& timeoutHandler) {\n  auto timeoutFunc = [this] {\n    if (!try_wait()) {\n      postHelper(TIMEOUT);\n    }\n  };\n  timeoutHandler.timeoutFunc_ = std::ref(timeoutFunc);\n  timeoutHandler.fiberManager_ = FiberManager::getFiberManagerUnsafe();\n  wait();\n  timeoutHandler.cancelTimeout();\n}\n\nvoid Baton::waitThread() {\n  auto waiter = waiter_.load();\n\n  auto waitStart = std::chrono::steady_clock::now();\n\n  if (LIKELY(\n          waiter == NO_WAITER &&\n          waiter_.compare_exchange_strong(waiter, THREAD_WAITING))) {\n    do {\n      folly::detail::MemoryIdler::futexWait(\n          futex_.futex, uint32_t(THREAD_WAITING));\n      waiter = waiter_.load(std::memory_order_acquire);\n    } while (waiter == THREAD_WAITING);\n  }\n\n  folly::async_tracing::logBlockingOperation(\n      std::chrono::duration_cast<std::chrono::milliseconds>(\n          std::chrono::steady_clock::now() - waitStart));\n\n  if (LIKELY(waiter == POSTED)) {\n    return;\n  }\n\n  // Handle errors\n  if (waiter == TIMEOUT) {\n    throw std::logic_error(\"Thread baton can't have timeout status\");\n  }\n  if (waiter == THREAD_WAITING) {\n    throw std::logic_error(\"Other thread is already waiting on this baton\");\n  }\n  throw std::logic_error(\"Other waiter is already waiting on this baton\");\n}\n\nvoid Baton::post() {\n  postHelper(POSTED);\n}\n\nvoid Baton::postHelper(intptr_t new_value) {\n  auto waiter = waiter_.load();\n\n  do {\n    if (waiter == THREAD_WAITING) {\n      assert(new_value == POSTED);\n\n      return postThread();\n    }\n\n    if (waiter == POSTED) {\n      return;\n    }\n  } while (!waiter_.compare_exchange_weak(waiter, new_value));\n\n  if (waiter != NO_WAITER && waiter != TIMEOUT) {\n    reinterpret_cast<Waiter*>(waiter)->post();\n  }\n}\n\nbool Baton::try_wait() {\n  return ready();\n}\n\nvoid Baton::postThread() {\n  auto expected = THREAD_WAITING;\n\n  auto* futex = &futex_.futex;\n  if (!waiter_.compare_exchange_strong(expected, POSTED)) {\n    return;\n  }\n  futexWake(futex, 1);\n}\n\nvoid Baton::reset() {\n  waiter_.store(NO_WAITER, std::memory_order_relaxed);\n}\n\nvoid Baton::TimeoutHandler::scheduleTimeout(std::chrono::milliseconds timeout) {\n  assert(fiberManager_ != nullptr);\n  assert(timeoutFunc_ != nullptr);\n\n  if (timeout.count() > 0) {\n    fiberManager_->loopController_->timer()->scheduleTimeout(this, timeout);\n  }\n}\n\n} // namespace fibers\n} // namespace folly\n","text":"Baton.cpp"}
{"code":"/*\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/fibers/Fiber.h>\n\n#include <algorithm>\n#include <cstring>\n#include <stdexcept>\n\n#include <glog/logging.h>\n\n#include <folly/Likely.h>\n#include <folly/Portability.h>\n#include <folly/fibers/FiberManagerInternal.h>\n#include <folly/portability/SysSyscall.h>\n#include <folly/portability/Unistd.h>\n\nnamespace folly {\nnamespace fibers {\n\nnamespace {\nconst uint64_t kMagic8Bytes = 0xfaceb00cfaceb00c;\n\nstd::thread::id localThreadId() {\n  return std::this_thread::get_id();\n}\n\n/* Size of the region from p + nBytes down to the last non-magic value */\nsize_t nonMagicInBytes(unsigned char* stackLimit, size_t stackSize) {\n  CHECK_EQ(reinterpret_cast<intptr_t>(stackLimit) % sizeof(uint64_t), 0u);\n  CHECK_EQ(stackSize % sizeof(uint64_t), 0u);\n  auto begin = reinterpret_cast<uint64_t*>(stackLimit);\n  auto end = reinterpret_cast<uint64_t*>(stackLimit + stackSize);\n\n  auto firstNonMagic = std::find_if(\n      begin, end, [](uint64_t val) { return val != kMagic8Bytes; });\n\n  return (end - firstNonMagic) * sizeof(uint64_t);\n}\n\n} // namespace\n\nvoid Fiber::resume() {\n  DCHECK_EQ(state_, AWAITING);\n  state_ = READY_TO_RUN;\n\n  if (LIKELY(threadId_ == localThreadId())) {\n    fiberManager_.readyFibers_.push_back(*this);\n    fiberManager_.ensureLoopScheduled();\n  } else {\n    fiberManager_.remoteReadyInsert(this);\n  }\n}\n\nFiber::Fiber(FiberManager& fiberManager)\n    : fiberManager_(fiberManager),\n      fiberStackSize_(fiberManager_.options_.stackSize),\n      fiberStackLimit_(fiberManager_.stackAllocator_.allocate(fiberStackSize_)),\n      fiberImpl_([this] { fiberFunc(); }, fiberStackLimit_, fiberStackSize_) {\n  fiberManager_.allFibers_.push_back(*this);\n\n#ifdef FOLLY_SANITIZE_THREAD\n  tsanCtx_ = __tsan_create_fiber(0);\n#endif\n}\n\nvoid Fiber::init(bool recordStackUsed) {\n// It is necessary to disable the logic for ASAN because we change\n// the fiber's stack.\n#ifndef FOLLY_SANITIZE_ADDRESS\n  recordStackUsed_ = recordStackUsed;\n  if (UNLIKELY(recordStackUsed_ && !stackFilledWithMagic_)) {\n    CHECK_EQ(\n        reinterpret_cast<intptr_t>(fiberStackLimit_) % sizeof(uint64_t), 0u);\n    CHECK_EQ(fiberStackSize_ % sizeof(uint64_t), 0u);\n    std::fill(\n        reinterpret_cast<uint64_t*>(fiberStackLimit_),\n        reinterpret_cast<uint64_t*>(fiberStackLimit_ + fiberStackSize_),\n        kMagic8Bytes);\n\n    stackFilledWithMagic_ = true;\n\n    // newer versions of boost allocate context on fiber stack,\n    // need to create a new one\n    fiberImpl_ =\n        FiberImpl([this] { fiberFunc(); }, fiberStackLimit_, fiberStackSize_);\n  }\n#else\n  (void)recordStackUsed;\n#endif\n}\n\nFiber::~Fiber() {\n#ifdef FOLLY_SANITIZE_ADDRESS\n  if (asanFakeStack_ != nullptr) {\n    fiberManager_.freeFakeStack(asanFakeStack_);\n  }\n  fiberManager_.unpoisonFiberStack(this);\n#endif\n\n#ifdef FOLLY_SANITIZE_THREAD\n  __tsan_destroy_fiber(tsanCtx_);\n#endif\n\n  fiberManager_.stackAllocator_.deallocate(fiberStackLimit_, fiberStackSize_);\n}\n\nvoid Fiber::recordStackPosition() {\n  // For ASAN builds, functions may run on fake stack.\n  // So we cannot get meaningful stack position.\n#ifndef FOLLY_SANITIZE_ADDRESS\n  int stackDummy;\n  auto currentPosition = static_cast<size_t>(\n      fiberStackLimit_ + fiberStackSize_ -\n      static_cast<unsigned char*>(static_cast<void*>(&stackDummy)));\n  fiberManager_.recordStackPosition(currentPosition);\n  VLOG(4) << \"Stack usage: \" << currentPosition;\n#endif\n}\n\n[[noreturn]] void Fiber::fiberFunc() {\n#ifdef FOLLY_SANITIZE_ADDRESS\n  fiberManager_.registerFinishSwitchStackWithAsan(\n      nullptr, &asanMainStackBase_, &asanMainStackSize_);\n#endif\n\n  while (true) {\n    DCHECK_EQ(state_, NOT_STARTED);\n\n    threadId_ = localThreadId();\n    if (taskOptions_.logRunningTime) {\n      prevDuration_ = std::chrono::microseconds(0);\n      currStartTime_ = thread_clock::now();\n    }\n    state_ = RUNNING;\n\n    try {\n      if (resultFunc_) {\n        DCHECK(finallyFunc_);\n        DCHECK(!func_);\n\n        resultFunc_();\n      } else {\n        DCHECK(func_);\n        func_();\n      }\n    } catch (...) {\n      fiberManager_.exceptionCallback_(\n          std::current_exception(), \"running Fiber func_/resultFunc_\");\n    }\n\n    if (UNLIKELY(recordStackUsed_)) {\n      auto newHighWatermark = fiberManager_.recordStackPosition(\n          nonMagicInBytes(fiberStackLimit_, fiberStackSize_));\n      VLOG(3) << \"Max stack usage: \" << newHighWatermark;\n      CHECK_LT(newHighWatermark, fiberManager_.options_.stackSize - 64)\n          << \"Fiber stack overflow\";\n    }\n\n    state_ = INVALID;\n\n    fiberManager_.deactivateFiber(this);\n  }\n}\n\nvoid Fiber::preempt(State state) {\n  auto preemptImpl = [&]() mutable {\n    DCHECK_EQ(fiberManager_.activeFiber_, this);\n    DCHECK_EQ(state_, RUNNING);\n    DCHECK_NE(state, RUNNING);\n    if (state != AWAITING_IMMEDIATE) {\n      CHECK(fiberManager_.currentException_ == std::current_exception());\n      CHECK_EQ(fiberManager_.numUncaughtExceptions_, uncaught_exceptions());\n    }\n\n    if (taskOptions_.logRunningTime) {\n      auto now = thread_clock::now();\n      prevDuration_ += now - currStartTime_;\n      currStartTime_ = now;\n    }\n    state_ = state;\n\n    recordStackPosition();\n\n    fiberManager_.deactivateFiber(this);\n\n    // Resumed from preemption\n    DCHECK_EQ(fiberManager_.activeFiber_, this);\n    DCHECK_EQ(state_, READY_TO_RUN);\n    if (taskOptions_.logRunningTime) {\n      currStartTime_ = thread_clock::now();\n    }\n    state_ = RUNNING;\n  };\n\n  if (fiberManager_.preemptRunner_) {\n    fiberManager_.preemptRunner_->run(std::ref(preemptImpl));\n  } else {\n    preemptImpl();\n  }\n}\n\nFiber::LocalData::~LocalData() {\n  reset();\n}\n\nFiber::LocalData::LocalData(const LocalData& other) : data_(nullptr) {\n  *this = other;\n}\n\nFiber::LocalData& Fiber::LocalData::operator=(const LocalData& other) {\n  reset();\n  if (!other.data_) {\n    return *this;\n  }\n\n  vtable_ = other.vtable_;\n  if (other.data_ == &other.buffer_) {\n    data_ = vtable_.ctor_copy(&buffer_, other.data_);\n  } else {\n    data_ = vtable_.make_copy(other.data_);\n  }\n\n  return *this;\n}\n\nvoid Fiber::LocalData::reset() {\n  if (!data_) {\n    return;\n  }\n\n  if (data_ == &buffer_) {\n    vtable_.dtor(data_);\n  } else {\n    vtable_.ruin(data_);\n  }\n  vtable_ = {};\n  data_ = nullptr;\n}\n\n} // namespace fibers\n} // namespace folly\n","text":"Fiber.cpp"}
{"code":"/*\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/fibers/FiberManagerInternal.h>\n\n#include <csignal>\n\n#include <cassert>\n#include <stdexcept>\n\n#include <glog/logging.h>\n\n#include <folly/fibers/Fiber.h>\n#include <folly/fibers/LoopController.h>\n\n#include <folly/ConstexprMath.h>\n#include <folly/SingletonThreadLocal.h>\n#include <folly/memory/SanitizeAddress.h>\n#include <folly/portability/Config.h>\n#include <folly/portability/SysSyscall.h>\n#include <folly/portability/Unistd.h>\n#include <folly/synchronization/SanitizeThread.h>\n\nnamespace std {\ntemplate <>\nstruct hash<folly::fibers::FiberManager::Options> {\n  ssize_t operator()(const folly::fibers::FiberManager::Options& opts) const {\n    return hash<decltype(opts.hash())>()(opts.hash());\n  }\n};\n} // namespace std\n\nnamespace folly {\nnamespace fibers {\n\nvoid FiberManager::defaultExceptionCallback(\n    const std::exception_ptr& eptr, StringPiece context) {\n  LOG(DFATAL) << \"Exception thrown in FiberManager with context '\" << context\n              << \"': \" << exceptionStr(eptr);\n}\n\nauto FiberManager::FrozenOptions::create(const Options& options) -> ssize_t {\n  return std::hash<Options>()(options);\n}\n\n/* static */ FiberManager*& FiberManager::getCurrentFiberManager() {\n  struct Tag {};\n  folly::annotate_ignore_thread_sanitizer_guard g(__FILE__, __LINE__);\n  return SingletonThreadLocal<FiberManager*, Tag>::get();\n}\n\nFiberManager::FiberManager(\n    std::unique_ptr<LoopController> loopController, Options options)\n    : FiberManager(LocalType<void>(), std::move(loopController), options) {}\n\nFiberManager::~FiberManager() {\n  loopController_.reset();\n\n  while (!fibersPool_.empty()) {\n    fibersPool_.pop_front_and_dispose([](Fiber* fiber) { delete fiber; });\n  }\n  assert(readyFibers_.empty());\n  assert(!hasTasks());\n}\n\nLoopController& FiberManager::loopController() {\n  return *loopController_;\n}\n\nconst LoopController& FiberManager::loopController() const {\n  return *loopController_;\n}\n\nbool FiberManager::hasTasks() const {\n  return fibersActive_.load(std::memory_order_relaxed) > 0 ||\n      !remoteReadyQueue_.empty() || !remoteTaskQueue_.empty() ||\n      remoteCount_ > 0;\n}\n\nbool FiberManager::isRemoteScheduled() const {\n  return remoteCount_ > 0;\n}\n\nFiber* FiberManager::getFiber() {\n  Fiber* fiber = nullptr;\n\n  if (options_.fibersPoolResizePeriodMs > 0 && !fibersPoolResizerScheduled_) {\n    fibersPoolResizer_.run();\n    fibersPoolResizerScheduled_ = true;\n  }\n\n  if (fibersPool_.empty()) {\n    fiber = new Fiber(*this);\n    fibersAllocated_.store(fibersAllocated() + 1, std::memory_order_relaxed);\n  } else {\n    fiber = &fibersPool_.front();\n    fibersPool_.pop_front();\n    auto fibersPoolSize = fibersPoolSize_.load(std::memory_order_relaxed);\n    assert(fibersPoolSize > 0);\n    fibersPoolSize_.store(fibersPoolSize - 1, std::memory_order_relaxed);\n  }\n  assert(fiber);\n  auto active = 1 + fibersActive_.fetch_add(1, std::memory_order_relaxed);\n  if (active > maxFibersActiveLastPeriod_) {\n    maxFibersActiveLastPeriod_ = active;\n  }\n  ++fiberId_;\n  bool recordStack = (options_.recordStackEvery != 0) &&\n      (fiberId_ % options_.recordStackEvery == 0);\n  fiber->init(recordStack);\n  return fiber;\n}\n\nvoid FiberManager::setExceptionCallback(FiberManager::ExceptionCallback ec) {\n  assert(ec);\n  exceptionCallback_ = std::move(ec);\n}\n\nsize_t FiberManager::fibersAllocated() const {\n  return fibersAllocated_.load(std::memory_order_relaxed);\n}\n\nsize_t FiberManager::fibersPoolSize() const {\n  return fibersPoolSize_.load(std::memory_order_relaxed);\n}\n\nsize_t FiberManager::stackHighWatermark() const {\n  return stackHighWatermark_.load(std::memory_order_relaxed);\n}\n\nvoid FiberManager::remoteReadyInsert(Fiber* fiber) {\n  if (remoteReadyQueue_.insertHead(fiber)) {\n    loopController_->scheduleThreadSafe();\n  }\n}\n\nvoid FiberManager::addObserver(ExecutionObserver* observer) {\n  observerList_.push_back(*observer);\n}\n\nvoid FiberManager::removeObserver(ExecutionObserver* observer) {\n  observerList_.erase(observerList_.iterator_to(*observer));\n}\n\nExecutionObserver::List& FiberManager::getObserverList() {\n  return observerList_;\n}\n\nvoid FiberManager::setPreemptRunner(InlineFunctionRunner* preemptRunner) {\n  preemptRunner_ = preemptRunner;\n}\n\nvoid FiberManager::doFibersPoolResizing() {\n  while (true) {\n    auto fibersAllocated = this->fibersAllocated();\n    auto fibersPoolSize = this->fibersPoolSize();\n    if (!(fibersAllocated > maxFibersActiveLastPeriod_ &&\n          fibersPoolSize > options_.maxFibersPoolSize)) {\n      break;\n    }\n    auto fiber = &fibersPool_.front();\n    assert(fiber != nullptr);\n    fibersPool_.pop_front();\n    delete fiber;\n    fibersPoolSize_.store(fibersPoolSize - 1, std::memory_order_relaxed);\n    fibersAllocated_.store(fibersAllocated - 1, std::memory_order_relaxed);\n  }\n\n  maxFibersActiveLastPeriod_ = fibersActive_.load(std::memory_order_relaxed);\n}\n\nvoid FiberManager::FibersPoolResizer::run() {\n  fiberManager_.doFibersPoolResizing();\n  if (auto timer = fiberManager_.loopController_->timer()) {\n    RequestContextScopeGuard rctxGuard(std::shared_ptr<RequestContext>{});\n    timer->scheduleTimeout(\n        this,\n        std::chrono::milliseconds(\n            fiberManager_.options_.fibersPoolResizePeriodMs));\n  }\n}\n\nvoid FiberManager::registerStartSwitchStackWithAsan(\n    void** saveFakeStack, const void* stackBottom, size_t stackSize) {\n  sanitizer_start_switch_fiber(saveFakeStack, stackBottom, stackSize);\n}\n\nvoid FiberManager::registerFinishSwitchStackWithAsan(\n    void* saveFakeStack, const void** saveStackBottom, size_t* saveStackSize) {\n  sanitizer_finish_switch_fiber(saveFakeStack, saveStackBottom, saveStackSize);\n}\n\nvoid FiberManager::freeFakeStack(void* fakeStack) {\n  void* saveFakeStack;\n  const void* stackBottom;\n  size_t stackSize;\n  sanitizer_start_switch_fiber(&saveFakeStack, nullptr, 0);\n  sanitizer_finish_switch_fiber(fakeStack, &stackBottom, &stackSize);\n  sanitizer_start_switch_fiber(nullptr, stackBottom, stackSize);\n  sanitizer_finish_switch_fiber(saveFakeStack, nullptr, nullptr);\n}\n\nvoid FiberManager::unpoisonFiberStack(const Fiber* fiber) {\n  auto stack = fiber->getStack();\n  asan_unpoison_memory_region(stack.first, stack.second);\n}\n\n// TVOS and WatchOS platforms have SIGSTKSZ but not sigaltstack\n#if defined(SIGSTKSZ) && !FOLLY_APPLE_TVOS && !FOLLY_APPLE_WATCHOS\n\nnamespace {\n\nbool hasAlternateStack() {\n  stack_t ss;\n  sigaltstack(nullptr, &ss);\n  return !(ss.ss_flags & SS_DISABLE);\n}\n\nint setAlternateStack(char* sp, size_t size) {\n  CHECK(sp);\n  stack_t ss{};\n  ss.ss_sp = sp;\n  ss.ss_size = size;\n  return sigaltstack(&ss, nullptr);\n}\n\nint unsetAlternateStack() {\n  stack_t ss{};\n  ss.ss_flags = SS_DISABLE;\n  return sigaltstack(&ss, nullptr);\n}\n\nclass ScopedAlternateSignalStack {\n public:\n  ScopedAlternateSignalStack() {\n    if (hasAlternateStack()) {\n      return;\n    }\n\n    // SIGSTKSZ (8 kB on our architectures) isn't always enough for\n    // folly::symbolizer, so allocate 32 kB.\n    size_t kAltStackSize = std::max(size_t(SIGSTKSZ), size_t(32 * 1024));\n\n    stack_ = std::unique_ptr<char[]>(new char[kAltStackSize]);\n\n    setAlternateStack(stack_.get(), kAltStackSize);\n  }\n\n  ScopedAlternateSignalStack(ScopedAlternateSignalStack&&) = default;\n  ScopedAlternateSignalStack& operator=(ScopedAlternateSignalStack&&) = default;\n\n  ~ScopedAlternateSignalStack() {\n    if (stack_) {\n      unsetAlternateStack();\n    }\n  }\n\n private:\n  std::unique_ptr<char[]> stack_;\n};\n} // namespace\n\nvoid FiberManager::maybeRegisterAlternateSignalStack() {\n  SingletonThreadLocal<ScopedAlternateSignalStack>::get();\n\n  alternateSignalStackRegistered_ = true;\n}\n\n#else\n\nvoid FiberManager::maybeRegisterAlternateSignalStack() {\n  // no-op\n}\n\n#endif\n\n} // namespace fibers\n} // namespace folly\n","text":"FiberManager.cpp"}
{"code":"/*\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/fibers/GuardPageAllocator.h>\n\n#ifndef _WIN32\n#include <dlfcn.h>\n#endif\n\n#include <csignal>\n#include <iostream>\n#include <mutex>\n\n#include <glog/logging.h>\n\n#include <folly/Singleton.h>\n#include <folly/SpinLock.h>\n#include <folly/Synchronized.h>\n#include <folly/portability/SysMman.h>\n#include <folly/portability/Unistd.h>\n\nnamespace folly {\nnamespace fibers {\n\n/**\n * Each stack with a guard page creates two memory mappings.\n * Since this is a limited resource, we don't want to create too many of these.\n *\n * The upper bound on total number of mappings created\n * is kNumGuarded * kMaxInUse.\n */\n\n/**\n * Number of guarded stacks per allocator instance\n */\nconstexpr size_t kNumGuarded = 100;\n\n/**\n * Maximum number of allocator instances with guarded stacks enabled\n */\nconstexpr size_t kMaxInUse = 100;\n\n/**\n * A cache for kNumGuarded stacks of a given size\n *\n * Thread safe.\n */\nclass StackCache {\n public:\n  explicit StackCache(size_t stackSize, size_t guardPagesPerStack)\n      : allocSize_(allocSize(stackSize, guardPagesPerStack)),\n        guardPagesPerStack_(guardPagesPerStack) {\n    auto p = ::mmap(\n        nullptr,\n        allocSize_ * kNumGuarded,\n        PROT_READ | PROT_WRITE,\n        MAP_PRIVATE | MAP_ANONYMOUS,\n        -1,\n        0);\n    PCHECK(p != (void*)(-1));\n    storage_ = reinterpret_cast<unsigned char*>(p);\n\n    /* Protect the bottommost page of every stack allocation */\n    freeList_.reserve(kNumGuarded);\n    for (size_t i = 0; i < kNumGuarded; ++i) {\n      auto allocBegin = storage_ + allocSize_ * i;\n      freeList_.emplace_back(allocBegin, /* protected= */ false);\n    }\n  }\n\n  unsigned char* borrow(size_t size) {\n    std::lock_guard<folly::SpinLock> lg(lock_);\n\n    assert(storage_);\n\n    auto as = allocSize(size, guardPagesPerStack_);\n    if (as != allocSize_ || freeList_.empty()) {\n      return nullptr;\n    }\n\n    auto p = freeList_.back().first;\n    if (!freeList_.back().second) {\n      PCHECK(0 == ::mprotect(p, pagesize() * guardPagesPerStack_, PROT_NONE));\n      protectedRanges().wlock()->insert(std::make_pair(\n          reinterpret_cast<intptr_t>(p),\n          reinterpret_cast<intptr_t>(p + pagesize() * guardPagesPerStack_)));\n    }\n    freeList_.pop_back();\n\n    /* We allocate minimum number of pages required, plus a guard page.\n       Since we use this for stack storage, requested allocation is aligned\n       at the top of the allocated pages, while the guard page is at the bottom.\n\n               -- increasing addresses -->\n             Guard page     Normal pages\n            |xxxxxxxxxx|..........|..........|\n            <- allocSize_ ------------------->\n         p -^                <- size -------->\n                      limit -^\n    */\n    auto limit = p + allocSize_ - size;\n    assert(limit >= p + pagesize() * guardPagesPerStack_);\n    return limit;\n  }\n\n  bool giveBack(unsigned char* limit, size_t size) {\n    std::lock_guard<folly::SpinLock> lg(lock_);\n\n    assert(storage_);\n\n    auto as = allocSize(size, guardPagesPerStack_);\n    if (std::less_equal<void*>{}(limit, storage_) ||\n        std::less_equal<void*>{}(storage_ + allocSize_ * kNumGuarded, limit)) {\n      /* not mine */\n      return false;\n    }\n\n    auto p = limit + size - as;\n    assert(as == allocSize_);\n    assert((p - storage_) % allocSize_ == 0);\n    freeList_.emplace_back(p, /* protected= */ true);\n    return true;\n  }\n\n  ~StackCache() {\n    assert(storage_);\n    protectedRanges().withWLock([&](auto& ranges) {\n      for (const auto& item : freeList_) {\n        ranges.erase(std::make_pair(\n            reinterpret_cast<intptr_t>(item.first),\n            reinterpret_cast<intptr_t>(\n                item.first + pagesize() * guardPagesPerStack_)));\n      }\n    });\n    PCHECK(0 == ::munmap(storage_, allocSize_ * kNumGuarded));\n  }\n\n  static bool isProtected(intptr_t addr) {\n    // Use a read lock for reading.\n    return protectedRanges().withRLock([&](auto const& ranges) {\n      for (const auto& range : ranges) {\n        if (range.first <= addr && addr < range.second) {\n          return true;\n        }\n      }\n      return false;\n    });\n  }\n\n private:\n  folly::SpinLock lock_;\n  unsigned char* storage_{nullptr};\n  const size_t allocSize_{0};\n  const size_t guardPagesPerStack_{0};\n\n  /**\n   * LIFO free list. Each pair contains stack pointer and protected flag.\n   */\n  std::vector<std::pair<unsigned char*, bool>> freeList_;\n\n  static size_t pagesize() {\n    static const auto pagesize = size_t(sysconf(_SC_PAGESIZE));\n    return pagesize;\n  }\n\n  /**\n   * Returns a multiple of pagesize() enough to store size + a few guard pages\n   */\n  static size_t allocSize(size_t size, size_t guardPages) {\n    return pagesize() * ((size + pagesize() * guardPages - 1) / pagesize() + 1);\n  }\n\n  /**\n   * For each [b, e) range in this set, the bytes in the range were mprotected.\n   */\n  static folly::Synchronized<std::unordered_set<std::pair<intptr_t, intptr_t>>>&\n  protectedRanges() {\n    static auto instance = new folly::Synchronized<\n        std::unordered_set<std::pair<intptr_t, intptr_t>>>();\n    return *instance;\n  }\n};\n\n#ifndef _WIN32\n\nnamespace {\n\nstruct sigaction oldSigsegvAction;\n\nFOLLY_NOINLINE void FOLLY_FIBERS_STACK_OVERFLOW_DETECTED(\n    int signum, siginfo_t* info, void* ucontext) {\n  std::cerr << \"folly::fibers Fiber stack overflow detected.\" << std::endl;\n  // Let the old signal handler handle the signal, but make this function name\n  // present in the stack trace.\n  if (oldSigsegvAction.sa_flags & SA_SIGINFO) {\n    oldSigsegvAction.sa_sigaction(signum, info, ucontext);\n  } else {\n    oldSigsegvAction.sa_handler(signum);\n  }\n  // Prevent tail call optimization.\n  std::cerr << \"\";\n}\n\nvoid sigsegvSignalHandler(int signum, siginfo_t* info, void* ucontext) {\n  // Restore old signal handler\n  sigaction(signum, &oldSigsegvAction, nullptr);\n\n  if (signum != SIGSEGV) {\n    std::cerr << \"GuardPageAllocator signal handler called for signal: \"\n              << signum;\n    return;\n  }\n\n  if (info &&\n      StackCache::isProtected(reinterpret_cast<intptr_t>(info->si_addr))) {\n    FOLLY_FIBERS_STACK_OVERFLOW_DETECTED(signum, info, ucontext);\n    return;\n  }\n\n  // Let the old signal handler handle the signal. Invoke this synchronously\n  // within our own signal handler to ensure that the kernel siginfo context\n  // is not lost.\n  if (oldSigsegvAction.sa_flags & SA_SIGINFO) {\n    oldSigsegvAction.sa_sigaction(signum, info, ucontext);\n  } else {\n    oldSigsegvAction.sa_handler(signum);\n  }\n}\n\nbool isInJVM() {\n  auto getCreated = dlsym(RTLD_DEFAULT, \"JNI_GetCreatedJavaVMs\");\n  return getCreated;\n}\n\nvoid installSignalHandler() {\n  static std::once_flag onceFlag;\n  std::call_once(onceFlag, []() {\n    if (isInJVM()) {\n      // Don't install signal handler, since JVM internal signal handler doesn't\n      // work with SA_ONSTACK\n      return;\n    }\n\n    struct sigaction sa;\n    memset(&sa, 0, sizeof(sa));\n    sigemptyset(&sa.sa_mask);\n    // By default signal handlers are run on the signaled thread's stack.\n    // In case of stack overflow running the SIGSEGV signal handler on\n    // the same stack leads to another SIGSEGV and crashes the program.\n    // Use SA_ONSTACK, so alternate stack is used (only if configured via\n    // sigaltstack).\n    sa.sa_flags |= SA_SIGINFO | SA_ONSTACK;\n    sa.sa_sigaction = &sigsegvSignalHandler;\n    sigaction(SIGSEGV, &sa, &oldSigsegvAction);\n  });\n}\n} // namespace\n\n#endif\n\nclass CacheManager {\n public:\n  static CacheManager& instance() {\n    static auto inst = new CacheManager();\n    return *inst;\n  }\n\n  std::unique_ptr<StackCacheEntry> getStackCache(\n      size_t stackSize, size_t guardPagesPerStack) {\n    auto used = inUse_.load(std::memory_order_relaxed);\n    do {\n      if (used >= kMaxInUse) {\n        return nullptr;\n      }\n    } while (!inUse_.compare_exchange_weak(\n        used, used + 1, std::memory_order_acquire, std::memory_order_relaxed));\n    return std::make_unique<StackCacheEntry>(stackSize, guardPagesPerStack);\n  }\n\n private:\n  std::atomic<size_t> inUse_{0};\n\n  friend class StackCacheEntry;\n\n  void giveBack(std::unique_ptr<StackCache> /* stackCache_ */) {\n    FOLLY_MAYBE_UNUSED auto wasUsed =\n        inUse_.fetch_sub(1, std::memory_order_release);\n    assert(wasUsed > 0);\n    /* Note: we can add a free list for each size bucket\n       if stack re-use is important.\n       In this case this needs to be a folly::Singleton\n       to make sure the free list is cleaned up on fork.\n\n       TODO(t7351705): fix Singleton destruction order\n    */\n  }\n};\n\n/*\n * RAII Wrapper around a StackCache that calls\n * CacheManager::giveBack() on destruction.\n */\nclass StackCacheEntry {\n public:\n  explicit StackCacheEntry(size_t stackSize, size_t guardPagesPerStack)\n      : stackCache_(\n            std::make_unique<StackCache>(stackSize, guardPagesPerStack)) {}\n\n  StackCache& cache() const noexcept { return *stackCache_; }\n\n  ~StackCacheEntry() {\n    CacheManager::instance().giveBack(std::move(stackCache_));\n  }\n\n private:\n  std::unique_ptr<StackCache> stackCache_;\n};\n\nGuardPageAllocator::GuardPageAllocator(size_t guardPagesPerStack)\n    : guardPagesPerStack_(guardPagesPerStack) {\n#ifndef _WIN32\n  installSignalHandler();\n#endif\n}\n\nGuardPageAllocator::~GuardPageAllocator() = default;\n\nunsigned char* GuardPageAllocator::allocate(size_t size) {\n  if (guardPagesPerStack_ && !stackCache_) {\n    stackCache_ =\n        CacheManager::instance().getStackCache(size, guardPagesPerStack_);\n  }\n\n  if (stackCache_) {\n    auto p = stackCache_->cache().borrow(size);\n    if (p != nullptr) {\n      return p;\n    }\n  }\n  return fallbackAllocator_.allocate(size);\n}\n\nvoid GuardPageAllocator::deallocate(unsigned char* limit, size_t size) {\n  if (!(stackCache_ && stackCache_->cache().giveBack(limit, size))) {\n    fallbackAllocator_.deallocate(limit, size);\n  }\n}\n} // namespace fibers\n} // namespace folly\n","text":"GuardPageAllocator.cpp"}
{"code":"/*\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/fibers/Semaphore.h>\n\nnamespace folly {\nnamespace fibers {\n\nbool Semaphore::signalSlow() {\n  Waiter* waiter = nullptr;\n  {\n    // If we signalled a release, notify the waitlist\n    auto waitListLock = waitList_.wlock();\n    auto& waitList = *waitListLock;\n\n    auto testVal = tokens_.load(std::memory_order_acquire);\n    if (testVal != 0) {\n      return false;\n    }\n\n    if (waitList.empty()) {\n      // If the waitlist is now empty, ensure the token count increments\n      // No need for CAS here as we will always be under the mutex\n      CHECK(tokens_.compare_exchange_strong(\n          testVal, testVal + 1, std::memory_order_relaxed));\n      return true;\n    }\n    waiter = &waitList.front();\n    waitList.pop_front();\n  }\n  // Trigger waiter if there is one\n  // Do it after releasing the waitList mutex, in case the waiter\n  // eagerly calls signal\n  waiter->baton.post();\n  return true;\n}\n\nvoid Semaphore::signal() {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal == 0) {\n      if (signalSlow()) {\n        return;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal + 1,\n      std::memory_order_release,\n      std::memory_order_acquire));\n}\n\nbool Semaphore::waitSlow(Waiter& waiter) {\n  // Slow path, create a baton and acquire a mutex to update the wait list\n  {\n    auto waitListLock = waitList_.wlock();\n    auto& waitList = *waitListLock;\n\n    auto testVal = tokens_.load(std::memory_order_acquire);\n    if (testVal != 0) {\n      return false;\n    }\n    // prepare baton and add to queue\n    waitList.push_back(waiter);\n    assert(!waitList.empty());\n  }\n  // Signal to caller that we managed to push a waiter\n  return true;\n}\n\nvoid Semaphore::wait() {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal == 0) {\n      Waiter waiter;\n      // If waitSlow fails it is because the token is non-zero by the time\n      // the lock is taken, so we can just continue round the loop\n      if (waitSlow(waiter)) {\n        waiter.baton.wait();\n        return;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - 1,\n      std::memory_order_release,\n      std::memory_order_acquire));\n}\n\nbool Semaphore::try_wait(Waiter& waiter) {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal == 0) {\n      if (waitSlow(waiter)) {\n        return false;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - 1,\n      std::memory_order_release,\n      std::memory_order_acquire));\n  return true;\n}\n\nbool Semaphore::try_wait() {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    if (oldVal == 0) {\n      return false;\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - 1,\n      std::memory_order_release,\n      std::memory_order_acquire));\n  return true;\n}\n\n#if FOLLY_HAS_COROUTINES\n\ncoro::Task<void> Semaphore::co_wait() {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal == 0) {\n      Waiter waiter;\n      // If waitSlow fails it is because the token is non-zero by the time\n      // the lock is taken, so we can just continue round the loop\n      if (waitSlow(waiter)) {\n        bool cancelled = false;\n        {\n          const auto& ct = co_await folly::coro::co_current_cancellation_token;\n          folly::CancellationCallback cb{\n              ct, [&] {\n                {\n                  auto waitListLock = waitList_.wlock();\n                  auto& waitList = *waitListLock;\n\n                  if (!waiter.hook_.is_linked()) {\n                    // Already dequeued by signalSlow()\n                    return;\n                  }\n\n                  cancelled = true;\n                  waitList.erase(waitList.iterator_to(waiter));\n                }\n\n                waiter.baton.post();\n              }};\n\n          co_await waiter.baton;\n        }\n\n        // Check 'cancelled' flag only after deregistering the callback so we're\n        // sure that we aren't reading it concurrently with a potential write\n        // from a thread requesting cancellation.\n        if (cancelled) {\n          co_yield folly::coro::co_cancelled;\n        }\n\n        co_return;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - 1,\n      std::memory_order_release,\n      std::memory_order_acquire));\n}\n\n#endif\n\nnamespace {\n\nclass FutureWaiter final : public fibers::Baton::Waiter {\n public:\n  FutureWaiter() { semaphoreWaiter.baton.setWaiter(*this); }\n\n  void post() override {\n    std::unique_ptr<FutureWaiter> destroyOnReturn{this};\n    promise.setValue();\n  }\n\n  Semaphore::Waiter semaphoreWaiter;\n  folly::Promise<Unit> promise;\n};\n\n} // namespace\n\nSemiFuture<Unit> Semaphore::future_wait() {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal == 0) {\n      auto batonWaiterPtr = std::make_unique<FutureWaiter>();\n      // If waitSlow fails it is because the token is non-zero by the time\n      // the lock is taken, so we can just continue round the loop\n      auto future = batonWaiterPtr->promise.getSemiFuture();\n      if (waitSlow(batonWaiterPtr->semaphoreWaiter)) {\n        (void)batonWaiterPtr.release();\n        return future;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - 1,\n      std::memory_order_release,\n      std::memory_order_acquire));\n  return makeSemiFuture();\n}\n\nsize_t Semaphore::getCapacity() const {\n  return capacity_;\n}\n\nsize_t Semaphore::getAvailableTokens() const {\n  return tokens_.load(std::memory_order_relaxed);\n}\n\n} // namespace fibers\n} // namespace folly\n","text":"Semaphore.cpp"}
{"code":"/*\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/fibers/SemaphoreBase.h>\n\nnamespace folly {\nnamespace fibers {\n\nbool SemaphoreBase::signalSlow(int64_t tokens) {\n  do {\n    auto testVal = tokens_.load(std::memory_order_acquire);\n\n    Waiter* waiter = nullptr;\n    {\n      // If we signalled a release, notify the waitlist\n      auto waitListLock = waitList_.wlock();\n      auto& waitList = *waitListLock;\n\n      if (waitList.empty() || waitList.front().tokens_ > testVal + tokens) {\n        // If the waitlist is now empty or not enough tokens to resume next in a\n        // waitlist, ensure the token count increments. No need for CAS here as\n        // we will always be under the mutex\n        if (tokens_.compare_exchange_strong(\n                testVal, testVal + tokens, std::memory_order_relaxed)) {\n          return true;\n        }\n        continue;\n      }\n\n      waiter = &waitList.front();\n\n      // Check for tokens shortage and keep the waiter until tokens acquired\n      int64_t release =\n          (testVal > waiter->tokens_) ? 0 : waiter->tokens_ - testVal;\n      if (!tokens_.compare_exchange_strong(\n              testVal,\n              testVal + release - waiter->tokens_,\n              std::memory_order_relaxed)) {\n        continue;\n      }\n\n      tokens -= release;\n      waitList.pop_front();\n    }\n\n    // Trigger waiter if there is one\n    // Do it after releasing the waitList mutex, in case the waiter\n    // eagerly calls signal\n    waiter->baton.post();\n  } while (tokens > 0);\n\n  return true;\n}\n\nbool SemaphoreBase::waitSlow(Waiter& waiter, int64_t tokens) {\n  // Slow path, create a baton and acquire a mutex to update the wait list\n  {\n    auto waitListLock = waitList_.wlock();\n    auto& waitList = *waitListLock;\n\n    auto testVal = tokens_.load(std::memory_order_acquire);\n    if (testVal >= tokens) {\n      return false;\n    }\n    // prepare baton and add to queue\n    waitList.push_back(waiter);\n    assert(!waitList.empty());\n  }\n  // Signal to caller that we managed to push a waiter\n  return true;\n}\n\nvoid SemaphoreBase::wait_common(int64_t tokens) {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal < tokens) {\n      Waiter waiter{tokens};\n      // If waitSlow fails it is because the capacity is greater than\n      // requested by the time the lock is taken, so we can just continue\n      // round the loop\n      if (waitSlow(waiter, tokens)) {\n        waiter.baton.wait();\n        return;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - tokens,\n      std::memory_order_release,\n      std::memory_order_acquire));\n}\n\nbool SemaphoreBase::try_wait_common(Waiter& waiter, int64_t tokens) {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal < tokens) {\n      if (waitSlow(waiter, tokens)) {\n        return false;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - tokens,\n      std::memory_order_release,\n      std::memory_order_acquire));\n  return true;\n}\n\nbool SemaphoreBase::try_wait_common(int64_t tokens) {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    if (oldVal < tokens) {\n      return false;\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - tokens,\n      std::memory_order_release,\n      std::memory_order_acquire));\n  return true;\n}\n\n#if FOLLY_HAS_COROUTINES\n\ncoro::Task<void> SemaphoreBase::co_wait_common(int64_t tokens) {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal < tokens) {\n      Waiter waiter{tokens};\n      // If waitSlow fails it is because the capacity is greater than\n      // requested by the time the lock is taken, so we can just continue\n      // round the loop\n      if (waitSlow(waiter, tokens)) {\n        bool cancelled = false;\n        {\n          const auto& ct = co_await folly::coro::co_current_cancellation_token;\n          folly::CancellationCallback cb{\n              ct, [&] {\n                {\n                  auto waitListLock = waitList_.wlock();\n                  auto& waitList = *waitListLock;\n\n                  if (!waiter.hook_.is_linked()) {\n                    // Already dequeued by signalSlow()\n                    return;\n                  }\n\n                  cancelled = true;\n                  waitList.erase(waitList.iterator_to(waiter));\n                }\n\n                waiter.baton.post();\n              }};\n\n          co_await waiter.baton;\n        }\n\n        // Check 'cancelled' flag only after deregistering the callback so\n        // we're sure that we aren't reading it concurrently with a potential\n        // write from a thread requesting cancellation.\n        if (cancelled) {\n          co_yield folly::coro::co_cancelled;\n        }\n\n        co_return;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - tokens,\n      std::memory_order_release,\n      std::memory_order_acquire));\n}\n\n#endif\n\nnamespace {\n\nclass FutureWaiter final : public fibers::Baton::Waiter {\n public:\n  explicit FutureWaiter(int64_t tokens) : semaphoreWaiter(tokens) {\n    semaphoreWaiter.baton.setWaiter(*this);\n  }\n\n  void post() override {\n    std::unique_ptr<FutureWaiter> destroyOnReturn{this};\n    promise.setValue();\n  }\n\n  SemaphoreBase::Waiter semaphoreWaiter;\n  folly::Promise<Unit> promise;\n};\n\n} // namespace\n\nSemiFuture<Unit> SemaphoreBase::future_wait_common(int64_t tokens) {\n  auto oldVal = tokens_.load(std::memory_order_acquire);\n  do {\n    while (oldVal < tokens) {\n      auto batonWaiterPtr = std::make_unique<FutureWaiter>(tokens);\n      // If waitSlow fails it is because the capacity is greater than\n      // requested by the time the lock is taken, so we can just continue\n      // round the loop\n      auto future = batonWaiterPtr->promise.getSemiFuture();\n      if (waitSlow(batonWaiterPtr->semaphoreWaiter, tokens)) {\n        (void)batonWaiterPtr.release();\n        return future;\n      }\n      oldVal = tokens_.load(std::memory_order_acquire);\n    }\n  } while (!tokens_.compare_exchange_weak(\n      oldVal,\n      oldVal - tokens,\n      std::memory_order_release,\n      std::memory_order_acquire));\n  return makeSemiFuture();\n}\n\nsize_t SemaphoreBase::getCapacity() const {\n  return capacity_;\n}\n\nsize_t SemaphoreBase::getAvailableTokens() const {\n  return tokens_.load(std::memory_order_relaxed);\n}\n\n} // namespace fibers\n} // namespace folly\n","text":"SemaphoreBase.cpp"}
{"code":"/*\n * Copyright (c) Meta Platforms, Inc. and affiliates.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <folly/fibers/SimpleLoopController.h>\n\n#include <folly/io/async/TimeoutManager.h>\n\nnamespace folly {\nnamespace fibers {\n\n/**\n * A simple version of TimeoutManager that maintains only a single AsyncTimeout\n * object that is used by HHWheelTimer in SimpleLoopController.\n */\nclass SimpleLoopController::SimpleTimeoutManager : public TimeoutManager {\n public:\n  explicit SimpleTimeoutManager(SimpleLoopController& loopController)\n      : loopController_(loopController) {}\n\n  void attachTimeoutManager(\n      AsyncTimeout* /* unused */, InternalEnum /* unused */) final {}\n  void detachTimeoutManager(AsyncTimeout* /* unused */) final {}\n\n  bool scheduleTimeout(AsyncTimeout* obj, timeout_type timeout) final {\n    // Make sure that we don't try to use this manager with two timeouts.\n    CHECK(!timeout_ || timeout_->first == obj);\n    timeout_.emplace(obj, std::chrono::steady_clock::now() + timeout);\n    return true;\n  }\n\n  void cancelTimeout(AsyncTimeout* obj) final {\n    CHECK(timeout_ && timeout_->first == obj);\n    timeout_.reset();\n  }\n\n  void bumpHandlingTime() final {}\n\n  bool isInTimeoutManagerThread() final {\n    return loopController_.isInLoopThread();\n  }\n\n  void runTimeouts() {\n    std::chrono::steady_clock::time_point tp = std::chrono::steady_clock::now();\n    if (!timeout_ || tp < timeout_->second) {\n      return;\n    }\n\n    auto* timeout = timeout_->first;\n    timeout_.reset();\n    timeout->timeoutExpired();\n  }\n\n private:\n  SimpleLoopController& loopController_;\n  folly::Optional<\n      std::pair<AsyncTimeout*, std::chrono::steady_clock::time_point>>\n      timeout_;\n};\n\nSimpleLoopController::SimpleLoopController()\n    : fm_(nullptr),\n      stopRequested_(false),\n      loopThread_(),\n      timeoutManager_(std::make_unique<SimpleTimeoutManager>(*this)),\n      timer_(HHWheelTimer::newTimer(timeoutManager_.get())) {}\n\nSimpleLoopController::~SimpleLoopController() {\n  scheduled_ = false;\n}\n\nvoid SimpleLoopController::runTimeouts() {\n  timeoutManager_->runTimeouts();\n}\n\n} // namespace fibers\n} // namespace folly\n","text":"SimpleLoopController.cpp"}
